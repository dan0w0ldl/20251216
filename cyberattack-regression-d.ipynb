{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-14T08:28:55.634152Z",
     "iopub.status.busy": "2025-12-14T08:28:55.633930Z",
     "iopub.status.idle": "2025-12-14T08:28:59.899266Z",
     "shell.execute_reply": "2025-12-14T08:28:59.898303Z",
     "shell.execute_reply.started": "2025-12-14T08:28:55.634110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:28:59.901643Z",
     "iopub.status.busy": "2025-12-14T08:28:59.901249Z",
     "iopub.status.idle": "2025-12-14T08:28:59.909553Z",
     "shell.execute_reply": "2025-12-14T08:28:59.908344Z",
     "shell.execute_reply.started": "2025-12-14T08:28:59.901620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:28:59.911080Z",
     "iopub.status.busy": "2025-12-14T08:28:59.910662Z",
     "iopub.status.idle": "2025-12-14T08:29:00.419604Z",
     "shell.execute_reply": "2025-12-14T08:29:00.418735Z",
     "shell.execute_reply.started": "2025-12-14T08:28:59.911050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv('train_data.csv')\n",
    "train_data_df.head()\n",
    "\n",
    "train_data_df.info()\n",
    "\n",
    "train_label_df = pd.read_csv(\"train_label.csv\")\n",
    "train_label_df.head()\n",
    "\n",
    "train_label_df = pd.read_csv(\"train_label.csv\")\n",
    "train_label_df.head()\n",
    "\n",
    "test_data_df = pd.read_csv('test_data.csv') \n",
    "test_data_df.head()\n",
    "\n",
    "test_label_df = pd.read_csv(\"sample_submission.csv\")\n",
    "test_label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 병합\n",
    "train = train_data_df.merge(train_label_df, on=\"Id\")\n",
    "test = test_data_df.copy()\n",
    "\n",
    "# 불필요 제거\n",
    "drop_cols = [\"Id\"]\n",
    "train.drop(columns=drop_cols, inplace=True)\n",
    "test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "#컬럼 그룹 정의\n",
    "target = \"Anomaly Scores\"\n",
    "\n",
    "numeric_cols = [\n",
    "    \"Source Port\",\n",
    "    \"Destination Port\",\n",
    "    \"Packet Length\"\n",
    "]\n",
    "\n",
    "high_cardinality_cols = [\n",
    "    \"Source IP Address\",\n",
    "    \"Destination IP Address\",\n",
    "    \"Attack Signature\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Protocol\",\n",
    "    \"Packet Type\",\n",
    "    \"Traffic Type\",\n",
    "    \"Malware Indicators\",\n",
    "    \"Alerts/Warnings\",\n",
    "    \"Action Taken\",\n",
    "    \"Severity Level\",\n",
    "    \"User Information\",\n",
    "    \"Device Information\",\n",
    "    \"Network Segment\",\n",
    "    \"Geo-location Data\",\n",
    "    \"Log Source\"\n",
    "]\n",
    "\n",
    "# 결측치 처리\n",
    "# 수치형\n",
    "for col in numeric_cols:\n",
    "    train[col] = train[col].fillna(train[col].median())\n",
    "    test[col] = test[col].fillna(train[col].median())\n",
    "\n",
    "# 문자열\n",
    "for col in categorical_cols + high_cardinality_cols:\n",
    "    train[col] = train[col].fillna(\"Unknown\")\n",
    "    test[col] = test[col].fillna(\"Unknown\")\n",
    "# 문자열 결측 → \"Unknown\"이라는 하나의 범주로 유지\n",
    "# 수치 결측 → 중앙값/(-1) + 필요 시 is_missing 플래그 가 낫다는 gpt의 판단\n",
    "\n",
    "\n",
    "#로그 변환 / Packet Length에 log1p 적용\n",
    "train[\"Packet_Length_log\"] = np.log1p(train[\"Packet Length\"])\n",
    "test[\"Packet_Length_log\"] = np.log1p(test[\"Packet Length\"])\n",
    "\n",
    "train.drop(columns=[\"Packet Length\"], inplace=True)\n",
    "test.drop(columns=[\"Packet Length\"], inplace=True)\n",
    "\n",
    "# 빈도 기반 인코딩 IP, Signature\n",
    "def frequency_encoding(train, test, col):\n",
    "    freq = train[col].value_counts()\n",
    "    train[col + \"_freq\"] = train[col].map(freq)\n",
    "    test[col + \"_freq\"] = test[col].map(freq)\n",
    "    test[col + \"_freq\"] = test[col + \"_freq\"].fillna(0)\n",
    "    return train, test\n",
    "\n",
    "for col in high_cardinality_cols:\n",
    "    train, test = frequency_encoding(train, test, col)\n",
    "\n",
    "\n",
    "train.drop(columns=high_cardinality_cols, inplace=True)\n",
    "test.drop(columns=high_cardinality_cols, inplace=True)\n",
    "\n",
    "\n",
    "# Severity, Actioin 규칙 기반 수치화(SOC 도메인 반영)\n",
    "severity_map = {\n",
    "    \"Low\": 0,\n",
    "    \"Medium\": 1,\n",
    "    \"High\": 2,\n",
    "    \"Critical\": 3\n",
    "}\n",
    "\n",
    "#Severity_num == Severity Level 수치화\n",
    "train[\"Severity_num\"] = train[\"Severity Level\"].map(severity_map).fillna(0)\n",
    "test[\"Severity_num\"] = test[\"Severity Level\"].map(severity_map).fillna(0)\n",
    "\n",
    "train[\"Action_Block\"] = (train[\"Action Taken\"] == \"Block\").astype(int)\n",
    "test[\"Action_Block\"] = (test[\"Action Taken\"] == \"Block\").astype(int)\n",
    "\n",
    "\n",
    "# 타깃 인코딩\n",
    "def target_encoding(train, test, col, target):\n",
    "    means = train.groupby(col)[target].mean()\n",
    "    train[col + \"_te\"] = train[col].map(means)\n",
    "    test[col + \"_te\"] = test[col].map(means)\n",
    "    global_mean = train[target].mean()\n",
    "    test[col + \"_te\"] = test[col + \"_te\"].fillna(global_mean)\n",
    "    return train, test\n",
    "\n",
    "te_cols = [\n",
    "    \"Protocol\",\n",
    "    \"Packet Type\",\n",
    "    \"Traffic Type\",\n",
    "    \"Alerts/Warnings\",\n",
    "    \"Malware Indicators\",\n",
    "    \"Network Segment\",\n",
    "    \"Log Source\"\n",
    "]\n",
    "\n",
    "for col in te_cols:\n",
    "    train, test = target_encoding(train, test, col, target)\n",
    "\n",
    "train.drop(columns=te_cols, inplace=True)\n",
    "test.drop(columns=te_cols, inplace=True)\n",
    "\n",
    "\n",
    "# 라벨 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_cols = train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for col in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train[col], test[col]])\n",
    "    le.fit(all_data)\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=[target])\n",
    "y = train[target]\n",
    "\n",
    "X_test = test.copy()\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state = random_state\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:00.742775Z",
     "iopub.status.busy": "2025-12-14T08:29:00.742355Z",
     "iopub.status.idle": "2025-12-14T08:29:00.912331Z",
     "shell.execute_reply": "2025-12-14T08:29:00.911389Z",
     "shell.execute_reply.started": "2025-12-14T08:29:00.742746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:00.939487Z",
     "iopub.status.busy": "2025-12-14T08:29:00.939216Z",
     "iopub.status.idle": "2025-12-14T08:29:00.982729Z",
     "shell.execute_reply": "2025-12-14T08:29:00.981620Z",
     "shell.execute_reply.started": "2025-12-14T08:29:00.939466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Validation RMSE: 28.88267\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1821\n",
      "[LightGBM] [Info] Number of data points in the train set: 25600, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 50.096059\n",
      "LGBM Validation RMSE: 28.90625\n",
      "\n",
      "Optimal Blending Weight (CatBoost): 0.75\n",
      "Optimal Blending Weight (LGBM): 0.25\n",
      "Blended Validation RMSE: 28.87879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=2000,          # 크게 잡고\n",
    "    learning_rate=0.03,         # 더 천천히\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=3000,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    random_seed=42,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_val, y_val)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(\n",
    "    X_train, y_train,\n",
    "    model__eval_set=[(X_val, y_val)],\n",
    "    model__eval_metric=\"rmse\",\n",
    "    model__callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Validation RMSE\n",
    "val_pred_cat = cat_model.predict(X_val)\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_val, val_pred_cat))\n",
    "print(f\"CatBoost Validation RMSE: {rmse_cat:.5f}\")\n",
    "\n",
    "test_pred_cat = cat_model.predict(X_test)\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"Anomaly Scores\"] = test_pred_cat\n",
    "submission.to_csv(\"test_submission_catboost.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:00.984226Z",
     "iopub.status.busy": "2025-12-14T08:29:00.983890Z",
     "iopub.status.idle": "2025-12-14T08:29:01.815075Z",
     "shell.execute_reply": "2025-12-14T08:29:01.813959Z",
     "shell.execute_reply.started": "2025-12-14T08:29:00.984197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:01.816547Z",
     "iopub.status.busy": "2025-12-14T08:29:01.815996Z",
     "iopub.status.idle": "2025-12-14T08:29:01.824705Z",
     "shell.execute_reply": "2025-12-14T08:29:01.823628Z",
     "shell.execute_reply.started": "2025-12-14T08:29:01.816514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target_scaler = StandardScaler()\n",
    "y_train = y_train.values\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.values\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_val_scaled = target_scaler.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:07.096459Z",
     "iopub.status.busy": "2025-12-14T08:29:07.096095Z",
     "iopub.status.idle": "2025-12-14T08:29:07.116806Z",
     "shell.execute_reply": "2025-12-14T08:29:07.115750Z",
     "shell.execute_reply.started": "2025-12-14T08:29:07.096429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 28.83369\n",
      "Validation RMSE: 28.90753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1213d\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\1213d\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "train_pred = pipeline.predict(X_train)\n",
    "val_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, train_pred)):.5f}\")\n",
    "print(f\"Validation RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:29:07.118426Z",
     "iopub.status.busy": "2025-12-14T08:29:07.118021Z",
     "iopub.status.idle": "2025-12-14T08:29:07.136615Z",
     "shell.execute_reply": "2025-12-14T08:29:07.135431Z",
     "shell.execute_reply.started": "2025-12-14T08:29:07.118402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 제출 파일 (test_submission_blended_optimized.csv) 생성이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# ❌ categorical_features, numerical_features, OneHotEncoder 전부 제거\n",
    "# ❌ 새로운 preprocessor 만들지 않음\n",
    "# ❌ lr, target_scaler 사용하지 않음\n",
    "\n",
    "# ===============================\n",
    "# 수정된 test submission 코드\n",
    "# ===============================\n",
    "\n",
    "# 이미 전처리 완료된 test 데이터 사용\n",
    "X_test = test.copy()   # ← 중요: test_data_df ❌, test ⭕\n",
    "\n",
    "\n",
    "# 이미 학습된 pipeline으로 예측\n",
    "test_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# sample_submission 기준으로 제출 파일 생성\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"Anomaly Scores\"] = test_pred\n",
    "\n",
    "\n",
    "# CSV 저장\n",
    "submission.to_csv(\"test_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14937917,
     "sourceId": 126123,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
